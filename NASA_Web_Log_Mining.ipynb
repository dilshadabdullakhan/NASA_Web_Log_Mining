{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Log Mining for Content Optimization using Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Web mining process can be defined as the process of applying data mining algorithms or techniques on web data so as to to discover the interesting or access patterns or knowledge to study user behaviour or user access patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Used:- NASA web access logs for July and August, 1995."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('float_format', '{:f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the gz files and import dataset provided in .TSV format.\n",
    "df1 = pd.read_csv(r\"19950801.00-19950901.00.tsv\", sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "df2 = pd.read_csv(r\"19950630.23-19950801.00.tsv\", sep=\"\\t\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine both the datasets.\n",
    "df = pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### EDA, Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check for columns with all null value\n",
    "df.isnull().values.all(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check for column with any null value\n",
    "df.isnull().values.any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### columns 'referer', 'useragent' contain all null values and 'logname' has all '-' values. Hence will be dropped\n",
    "df = df.loc[:,~df.columns.isin(['referer', 'useragent', 'logname'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniq_host = df.host.nunique()\n",
    "print(\"Unique hosts : {}\".format(uniq_host))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniq_url = df.url.nunique()\n",
    "print(\"Unique url : {}\".format(uniq_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create timestamp column by converting time variable to datetime format\n",
    "df['timestamp']= list(map(lambda x: datetime.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S'), df['time']))\n",
    "df=df.drop(columns = 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df.timestamp)\n",
    "df[\"date\"]=df['timestamp'].dt.strftime(\"%Y-%m-%d\")\n",
    "df[\"time\"]=df['timestamp'].dt.strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df.timestamp.dt.day\n",
    "df['month'] = df.timestamp.dt.month\n",
    "df['weekday']= df.timestamp.dt.day_name()\n",
    "df['hour']= df.timestamp.dt.hour\n",
    "df['minute']= df.timestamp.dt.minute\n",
    "df['second']= df.timestamp.dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count of rows for each month\n",
    "month_count = df['month'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It was mentioned in the description that data provided is for the month of July and August,1995. However we see that we have data for the month of september as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_day_month = df.groupby(['month','day'], as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique days in month 7 : 29\n",
      "unique days in month 8 : 31\n",
      "unique days in month 9 : 1\n"
     ]
    }
   ],
   "source": [
    "print('unique days in month 7 : {}'.format(count_day_month[count_day_month['month']==7]['day'].nunique()))\n",
    "print('unique days in month 8 : {}'.format(count_day_month[count_day_month['month']==8]['day'].nunique()))\n",
    "print('unique days in month 9 : {}'.format(count_day_month[count_day_month['month']==9]['day'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there is just one day data for the month of september, dropping it from the dataset\n",
    "df = df[df['month']!=9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 5 types of requested pages\n",
    "df['ext'] = df['url'].str.split('.').str[-1]\n",
    "df['ext'] = df['ext'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.5) #set the font of x and y tick labels\n",
    "df.ext.value_counts()[:5].plot.bar(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As per the above graph was can see that there are urls' with different extensions. Removing unwanted files or pages or URLs include removing urls's which are not required for a praticular use case scenario. Here we will keep only the urls' with  extension “.html” and remove records with other extensions like “gif”, “jpeg”, “css” and so on .\n",
    "\n",
    "###### Records with error status codes are also removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Status Messages\n",
    "\n",
    "###### 200:Successful:OK - The request is OK (this is the standard response for successful HTTP requests)\n",
    "###### 302:Redirection:Found: - The requested page has moved temporarily to a new URL \n",
    "###### 304:Redirection:Not Modified:-Indicates the requested page has not been modified since last requested\n",
    "###### 403:Client Error:Forbidden:-The request was a legal request, but the server is refusing to respond to it\n",
    "###### 404:Client Error:Not Found:-The requested page could not be found but may be available again in the future\n",
    "###### 500:Server Error:-Internal Server Error:-A generic error message, given when no more specific message is suitable\n",
    "###### 501:Server Error:Not Implemented:-The server either does not recognize the request method, or it lacks the ability to fulfill the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unwanted rows\n",
    "df_cleaned = df[(df['ext']=='html') & (df['response']<400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#customers access website mostly for which HTTP method?\n",
    "df_cleaned ['method'].value_counts().plot(kind='bar')\n",
    "plt.title(\"method type\")\n",
    "plt.xlabel(\"method\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend(['get','post','head'],loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#count the values of different HTTP mentods on clean dataset\n",
    "df_cleaned['method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#count the values of different response mentods on clean dataset\n",
    "df_cleaned.groupby('response').size().plot(kind='bar')\n",
    "plt.title(\"Response code count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Most of the requests are getting successful responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_bytes = df1_cleaned[df1_cleaned['bytes']==0]\n",
    "non_zero_bytes = df1_cleaned[df1['bytes']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot response code for zero bytes\n",
    "zero_bytes.groupby('response').size().plot(kind='bar')\n",
    "plt.title(\"Response code count zero_bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0 bytes response may be becuase the response is sent from cache of the browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot response code for non-zero bytes\n",
    "non_zero_bytes.groupby('response').size().plot(kind='bar')\n",
    "plt.title(\"Response code count non_zero_bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 hosts\n",
    "top_hosts = pd.DataFrame(df_cleaned.host.value_counts()[:10]).reset_index()\n",
    "top_hosts.columns = ['host', 'count']\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.barplot(x='count', y='host', data=top_hosts)\n",
    "ax.set_xlabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 accessed URLs'\n",
    "top_url = pd.DataFrame(df_cleaned.url.value_counts()[:10]).reset_index()\n",
    "top_url.columns = ['url', 'count']\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.barplot(x='count', y='url', data=top_url)\n",
    "ax.set_xlabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count unique daily hosts\n",
    "unq_daily_hosts= df_cleaned.groupby(['month','day'],as_index=False).agg({\"host\": 'count', \"host\":pd.Series.nunique})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(x=\"day\", \n",
    "            y=\"host\", \n",
    "            hue=\"month\", \n",
    "            data=unq_daily_hosts)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.xlabel(\"Day\", size=14)\n",
    "plt.title(\"Daily unique hosts\", size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count total daily hosts\n",
    "total_daily_hosts = df_cleaned.groupby(['month','day'], as_index=False)['host'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average daily request per host\n",
    "avg_daily_req_per_host = total_daily_hosts['host']/unq_daily_hosts['host']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(15,6 ))\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.xlabel(\"Day\", size=14)\n",
    "plt.plot(avg_daily_req_per_host)\n",
    "plt.title(\"Average Daily Request Per Host\", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_id(dataframe):\n",
    "    \n",
    "    '''\n",
    "      Names of hosts' and URLs' are very lengthy. To use them in the algorithm, and for easy interpretetion,\n",
    "      assigning unique IDs to Host and URL.\n",
    "      \n",
    "      Function takes in a dataframe with host and url name and returns complete dataframe with new columsn for ID's \n",
    "      along with other columns \n",
    "    '''\n",
    "    dataframe1 = dataframe.assign(host_id=(dataframe['host']).astype('category').cat.codes)\n",
    "    dataframe2 = dataframe1.assign(url_id=(dataframe['url']).astype('category').cat.codes)\n",
    "    dataframe2 = dataframe2.astype({\"host_id\":'category', \"url_id\":'category'})\n",
    "    \n",
    "    return dataframe2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = convert_to_id(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the dataset to select the required columns for algorithm input\n",
    "df_host_url = df_id[['date','url_id', 'host_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot the dataset\n",
    "\n",
    "df_host_url['idx'] = df_host_url.groupby('date').cumcount()\n",
    "df_host_url_pivot = df_host_url.pivot(index='date',columns='idx')[['url_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in range(0, 60): #no of rows\n",
    "    records.append([str(df_host_url_pivot.values[i,j]) for j in range(0, 10)]) #Here we are selecting only 10 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = apriori(records, min_support=0.05, min_lift=4, min_length=2)\n",
    "association_results = list(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_df = pd.DataFrame(columns=('Items','Antecedent','Consequent','Support','Confidence','Lift'))\n",
    "\n",
    "Support =[]\n",
    "Confidence = []\n",
    "Lift = []\n",
    "Items = []\n",
    "Antecedent = []\n",
    "Consequent=[]\n",
    "\n",
    "for RelationRecord in association_results:\n",
    "    for ordered_stat in RelationRecord.ordered_statistics:\n",
    "        Support.append(RelationRecord.support)\n",
    "        Items.append(RelationRecord.items)\n",
    "        Antecedent.append(ordered_stat.items_base)\n",
    "        Consequent.append(ordered_stat.items_add)\n",
    "        Confidence.append(ordered_stat.confidence)\n",
    "        Lift.append(ordered_stat.lift)\n",
    "\n",
    "sort_df['Items'] = list(map(set, Items))                                   \n",
    "sort_df['Antecedent'] = list(map(set, Antecedent))\n",
    "sort_df['Consequent'] = list(map(set, Consequent))\n",
    "sort_df['Support'] = Support\n",
    "sort_df['Confidence'] = Confidence\n",
    "sort_df['Lift']= Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_df.sort_values(by ='Lift', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination of Support, confidence and lift\n",
    "\n",
    "###### Support refers to the default popularity of an item and can be calculated by finding number of transactions containing a particular item  divided by total number of transactions\n",
    "###### Support(B) = (Transactions containing (B))/(Total Transactions)\n",
    "\n",
    "\n",
    "###### Confidence refers to the likelihood that an item B is also bought if item A is bought. It can be calculated by finding the number of transactions  where A and B are bought together, divided by total number of transactions where A is bought\n",
    "###### Confidence(A→B) = (Transactions containing both (A and B))/(Transactions containing A)\n",
    "\n",
    "\n",
    "###### Lift(A -> B) refers to the increase in the ratio of sale of B when A is sold. Lift(A –> B) can be calculated by dividing Confidence(A -> B) divided by Support(B)\n",
    "\n",
    "###### Lift(A→B) = (Confidence (A→B))/(Support (B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on patterns provided in the output, web content can be optimised by checking for content similarity."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
